{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "016033e8-14a1-4179-9d86-4adf70abcd76",
   "metadata": {},
   "source": [
    "# SSD Lite model on the cows dataset from ICAERUS/France\n",
    "Most of this is adapted from basic cookiecutter model training in pytorch.\n",
    "\n",
    "Dataset consists of three areas: Jalogny, Derval and Mauron: these directly correspond to train/test/val sets.\n",
    "I removed all images without an annotation in them (e.g. just a picture of a field, without a cow)\n",
    "* train: jalogny: x img, x annotations\n",
    "* test: derval: x imgs, x annotations\n",
    "* val: mauron: x imgs, x annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fc9da5-2fe5-4996-b8ec-46627ca5f26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d41757da-e949-4675-9aeb-fb2607e86833",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection import ssdlite320_mobilenet_v3_large\n",
    "from torchvision.datasets import VOCDetection\n",
    "from torchvision.transforms.v2 import functional as F\n",
    "from torchvision.transforms import v2\n",
    "from torchvision import ops\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torchvision import datapoints\n",
    "from torchvision.ops import generalized_box_iou_loss\n",
    "from torch.utils import tensorboard\n",
    "from torchvision.datasets import VisionDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46c699cb-b70c-4c48-b612-a6d2d6039ac6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class CustomDataset(VisionDataset):\n",
    "    def __init__(self, images, labels, boxes, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.boxes = boxes\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        labels = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        boxes = torch.tensor(self.boxes[idx], dtype = torch.float32)\n",
    "        target = {'boxes': boxes,\n",
    "                 'labels': labels}\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db6d09fb-7915-4c46-884f-11e6cbdffee6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# in the image_slicing.ipynb, the images are loaded and then tiled (in  a 320x320 grid), placed into a CustomDataset\n",
    "# with normalized values and all as tensors (), these are then pickled and stored.\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# Create a VOC dataset\n",
    "train_dataset = torch.load(\"data/train_set.pkl\")\n",
    "test_dataset = torch.load(\"data/test_set.pkl\")\n",
    "val_dataset = torch.load(\"data/val_set.pkl\")\n",
    "# Create a DataLoader for the VOC dataset\n",
    "batch_size = 8\n",
    "shuffle = True\n",
    "\n",
    "#and put them in the loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle, collate_fn = collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn = collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1fbbb5a-bf1d-4fb8-99c4-17ba3bac7f55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch Loss: 2.7929348945617676\n",
      "Epoch 1, Validation Loss: 2.7929370403289795\n",
      "Epoch 2, Batch Loss: 2.1547679901123047\n",
      "Epoch 3, Batch Loss: 2.3239827156066895\n",
      "Epoch 4, Batch Loss: 1.9026097059249878\n",
      "Epoch 5, Batch Loss: 2.445168972015381\n",
      "Epoch 6, Batch Loss: 2.473205327987671\n",
      "Epoch 7, Batch Loss: 2.8486196994781494\n",
      "Epoch 8, Batch Loss: 2.6673994064331055\n",
      "Epoch 9, Batch Loss: 1.0177251100540161\n",
      "Epoch 10, Batch Loss: 3.4130921363830566\n",
      "Epoch 11, Batch Loss: 2.5426130294799805\n",
      "Epoch 11, Validation Loss: 2.542614221572876\n",
      "Epoch 12, Batch Loss: 1.9889143705368042\n",
      "Epoch 13, Batch Loss: 2.547889232635498\n",
      "Epoch 14, Batch Loss: 1.6018751859664917\n",
      "Epoch 15, Batch Loss: 1.93035089969635\n",
      "Epoch 16, Batch Loss: 1.292243480682373\n",
      "Epoch 17, Batch Loss: 1.1661664247512817\n",
      "Epoch 18, Batch Loss: 1.805835247039795\n",
      "Epoch 19, Batch Loss: 1.842522382736206\n",
      "Epoch 20, Batch Loss: 1.6262757778167725\n",
      "Epoch 21, Batch Loss: 0.8940614461898804\n",
      "Epoch 21, Validation Loss: 0.8940613865852356\n",
      "Epoch 22, Batch Loss: 0.5614913105964661\n",
      "Epoch 23, Batch Loss: 1.022173523902893\n",
      "Epoch 24, Batch Loss: 1.201902985572815\n",
      "Epoch 25, Batch Loss: 0.9377862215042114\n",
      "Epoch 26, Batch Loss: 0.9194077849388123\n",
      "Epoch 27, Batch Loss: 0.4486415386199951\n",
      "Epoch 28, Batch Loss: 0.3573141396045685\n",
      "Epoch 29, Batch Loss: 0.40470990538597107\n",
      "Epoch 30, Batch Loss: 0.5858781337738037\n",
      "Epoch 31, Batch Loss: 0.5765154361724854\n",
      "Epoch 31, Validation Loss: 0.5765151977539062\n",
      "Epoch 32, Batch Loss: 0.23108261823654175\n",
      "Epoch 33, Batch Loss: 0.20502416789531708\n",
      "Epoch 34, Batch Loss: 0.2643694579601288\n",
      "Epoch 35, Batch Loss: 0.31375542283058167\n",
      "Epoch 36, Batch Loss: 0.20928174257278442\n",
      "Epoch 37, Batch Loss: 0.2629260718822479\n",
      "Epoch 38, Batch Loss: 0.17541450262069702\n",
      "Epoch 39, Batch Loss: 0.15838009119033813\n",
      "Epoch 40, Batch Loss: 0.18218304216861725\n",
      "Epoch 41, Batch Loss: 0.32352957129478455\n",
      "Epoch 41, Validation Loss: 0.3235297203063965\n",
      "Epoch 42, Batch Loss: 0.16169659793376923\n",
      "Epoch 43, Batch Loss: 0.929754912853241\n",
      "Epoch 44, Batch Loss: 0.4323863983154297\n",
      "Epoch 45, Batch Loss: 0.14785578846931458\n",
      "Epoch 46, Batch Loss: 0.16150301694869995\n",
      "Epoch 47, Batch Loss: 0.09312643855810165\n",
      "Epoch 48, Batch Loss: 0.13600952923297882\n",
      "Epoch 49, Batch Loss: 0.1323656141757965\n",
      "Epoch 50, Batch Loss: 0.17202462255954742\n",
      "Epoch 51, Batch Loss: 0.07971146702766418\n",
      "Epoch 51, Validation Loss: 0.0797114372253418\n",
      "Epoch 52, Batch Loss: 0.08250551670789719\n",
      "Epoch 53, Batch Loss: 0.1921561360359192\n",
      "Epoch 54, Batch Loss: 0.11331640183925629\n",
      "Epoch 55, Batch Loss: 0.18973486125469208\n",
      "Epoch 56, Batch Loss: 0.09065649658441544\n",
      "Epoch 57, Batch Loss: 0.07103573530912399\n",
      "Epoch 58, Batch Loss: 0.08615028858184814\n",
      "Epoch 59, Batch Loss: 0.13569268584251404\n",
      "Epoch 60, Batch Loss: 0.10833252221345901\n",
      "Epoch 61, Batch Loss: 0.2723511755466461\n",
      "Epoch 61, Validation Loss: 0.2723509669303894\n",
      "Epoch 62, Batch Loss: 0.0889132171869278\n",
      "Epoch 63, Batch Loss: 0.08396930247545242\n",
      "Epoch 64, Batch Loss: 0.060773707926273346\n",
      "Epoch 65, Batch Loss: 0.14656461775302887\n",
      "Epoch 66, Batch Loss: 0.06652510911226273\n",
      "Epoch 67, Batch Loss: 0.053005680441856384\n",
      "Epoch 68, Batch Loss: 0.08055620640516281\n",
      "Epoch 69, Batch Loss: 0.09828635305166245\n",
      "Epoch 70, Batch Loss: 0.05571505427360535\n",
      "Epoch 71, Batch Loss: 0.06556099653244019\n",
      "Epoch 71, Validation Loss: 0.06556098163127899\n",
      "Epoch 72, Batch Loss: 0.08047127723693848\n",
      "Epoch 73, Batch Loss: 0.06091982126235962\n",
      "Epoch 74, Batch Loss: 0.06990876793861389\n",
      "Epoch 75, Batch Loss: 0.06018241494894028\n",
      "Epoch 76, Batch Loss: 0.0437970831990242\n",
      "Epoch 77, Batch Loss: 0.03595713526010513\n",
      "Epoch 78, Batch Loss: 0.08003515005111694\n",
      "Epoch 79, Batch Loss: 0.06382197886705399\n",
      "Epoch 80, Batch Loss: 0.06755489110946655\n",
      "Epoch 81, Batch Loss: 0.17150917649269104\n",
      "Epoch 81, Validation Loss: 0.1715090423822403\n",
      "Epoch 82, Batch Loss: 0.0359545573592186\n",
      "Epoch 83, Batch Loss: 0.02810480259358883\n",
      "Epoch 84, Batch Loss: 0.027835294604301453\n",
      "Epoch 85, Batch Loss: 0.04713022708892822\n",
      "Epoch 86, Batch Loss: 0.03779216110706329\n",
      "Epoch 87, Batch Loss: 0.035411182790994644\n",
      "Epoch 88, Batch Loss: 0.03107617236673832\n",
      "Epoch 89, Batch Loss: 0.09918513894081116\n",
      "Epoch 90, Batch Loss: 0.0986780896782875\n",
      "Epoch 91, Batch Loss: 0.03568938001990318\n",
      "Epoch 91, Validation Loss: 0.03568935766816139\n",
      "Epoch 92, Batch Loss: 0.031423840671777725\n",
      "Epoch 93, Batch Loss: 0.04395132511854172\n",
      "Epoch 94, Batch Loss: 0.0512063205242157\n",
      "Epoch 95, Batch Loss: 0.03870163857936859\n",
      "Epoch 96, Batch Loss: 0.07905065268278122\n",
      "Epoch 97, Batch Loss: 0.033584412187337875\n",
      "Epoch 98, Batch Loss: 0.08839434385299683\n",
      "Epoch 99, Batch Loss: 0.041270311921834946\n",
      "Epoch 100, Batch Loss: 0.04935075715184212\n",
      "Epoch 101, Batch Loss: 0.03191643953323364\n",
      "Epoch 101, Validation Loss: 0.03191640228033066\n",
      "Epoch 102, Batch Loss: 0.03192761540412903\n",
      "Epoch 103, Batch Loss: 0.014010203070938587\n",
      "Epoch 104, Batch Loss: 0.02889014221727848\n",
      "Epoch 105, Batch Loss: 0.03552304953336716\n",
      "Epoch 106, Batch Loss: 0.0377684012055397\n",
      "Epoch 107, Batch Loss: 0.03170410543680191\n",
      "Epoch 108, Batch Loss: 0.02997434139251709\n",
      "Epoch 109, Batch Loss: 0.030123122036457062\n",
      "Epoch 110, Batch Loss: 0.02422131597995758\n",
      "Epoch 111, Batch Loss: 0.03477124869823456\n",
      "Epoch 111, Validation Loss: 0.03477122262120247\n",
      "Epoch 112, Batch Loss: 0.02015276812016964\n",
      "Epoch 113, Batch Loss: 0.056236382573843\n",
      "Epoch 114, Batch Loss: 0.024389969184994698\n",
      "Epoch 115, Batch Loss: 0.03635771945118904\n",
      "Epoch 116, Batch Loss: 0.020999720320105553\n",
      "Epoch 117, Batch Loss: 0.018443267792463303\n",
      "Epoch 118, Batch Loss: 0.022327227517962456\n",
      "Epoch 119, Batch Loss: 0.01695677824318409\n",
      "Epoch 120, Batch Loss: 0.016467172652482986\n",
      "Epoch 121, Batch Loss: 0.025961527600884438\n",
      "Epoch 121, Validation Loss: 0.02596152573823929\n",
      "Epoch 122, Batch Loss: 0.03445468842983246\n",
      "Epoch 123, Batch Loss: 0.03048068657517433\n",
      "Epoch 124, Batch Loss: 0.03087199106812477\n",
      "Epoch 125, Batch Loss: 0.03436753898859024\n",
      "Epoch 126, Batch Loss: 0.05072180554270744\n",
      "Epoch 127, Batch Loss: 0.06791862100362778\n",
      "Epoch 128, Batch Loss: 0.020968478173017502\n",
      "Epoch 129, Batch Loss: 0.035691022872924805\n",
      "Epoch 130, Batch Loss: 0.02140887640416622\n",
      "Epoch 131, Batch Loss: 0.03345708176493645\n",
      "Epoch 131, Validation Loss: 0.03345705568790436\n",
      "Epoch 132, Batch Loss: 0.012841811403632164\n",
      "Epoch 133, Batch Loss: 0.01654939167201519\n",
      "Epoch 134, Batch Loss: 0.02262015827000141\n",
      "Epoch 135, Batch Loss: 0.040727365761995316\n",
      "Epoch 136, Batch Loss: 0.03328622877597809\n",
      "Epoch 137, Batch Loss: 0.013090437278151512\n",
      "Epoch 138, Batch Loss: 0.008757366798818111\n",
      "Epoch 139, Batch Loss: 0.0259879007935524\n",
      "Epoch 140, Batch Loss: 0.018453659489750862\n",
      "Epoch 141, Batch Loss: 0.011532037518918514\n",
      "Epoch 141, Validation Loss: 0.011532023549079895\n",
      "Epoch 142, Batch Loss: 0.016449658200144768\n",
      "Epoch 143, Batch Loss: 0.03166215866804123\n",
      "Epoch 144, Batch Loss: 0.019858969375491142\n",
      "Epoch 145, Batch Loss: 0.02875290811061859\n",
      "Epoch 146, Batch Loss: 0.07180321216583252\n",
      "Epoch 147, Batch Loss: 0.03460574522614479\n",
      "Epoch 148, Batch Loss: 0.015040211379528046\n",
      "Epoch 149, Batch Loss: 0.028079397976398468\n",
      "Epoch 150, Batch Loss: 0.015244297683238983\n",
      "Epoch 151, Batch Loss: 0.022086862474679947\n",
      "Epoch 151, Validation Loss: 0.02208685874938965\n",
      "Epoch 152, Batch Loss: 0.02377818152308464\n",
      "Epoch 153, Batch Loss: 0.0189190823584795\n",
      "Epoch 154, Batch Loss: 0.03582090511918068\n",
      "Epoch 155, Batch Loss: 0.021904276683926582\n",
      "Epoch 156, Batch Loss: 0.023514261469244957\n",
      "Epoch 157, Batch Loss: 0.17886367440223694\n",
      "Epoch 158, Batch Loss: 0.02878759056329727\n",
      "Epoch 159, Batch Loss: 0.01408642902970314\n",
      "Epoch 160, Batch Loss: 0.022930851206183434\n",
      "Epoch 161, Batch Loss: 0.031143352389335632\n",
      "Epoch 161, Validation Loss: 0.031143352389335632\n",
      "Epoch 162, Batch Loss: 0.013491724617779255\n",
      "Epoch 163, Batch Loss: 0.028302686288952827\n",
      "Epoch 164, Batch Loss: 0.018067216500639915\n",
      "Epoch 165, Batch Loss: 0.02077123336493969\n",
      "Epoch 166, Batch Loss: 0.015850214287638664\n",
      "Epoch 167, Batch Loss: 0.013163436204195023\n",
      "Epoch 168, Batch Loss: 0.01562853902578354\n",
      "Epoch 169, Batch Loss: 0.021810943260788918\n",
      "Epoch 170, Batch Loss: 0.025137213990092278\n",
      "Epoch 171, Batch Loss: 0.015736591070890427\n",
      "Epoch 171, Validation Loss: 0.01573658175766468\n",
      "Epoch 172, Batch Loss: 0.014948743395507336\n",
      "Epoch 173, Batch Loss: 0.01772746443748474\n",
      "Epoch 174, Batch Loss: 0.01273361686617136\n",
      "Epoch 175, Batch Loss: 0.0316547267138958\n",
      "Epoch 176, Batch Loss: 0.010414798744022846\n",
      "Epoch 177, Batch Loss: 0.016633473336696625\n",
      "Epoch 178, Batch Loss: 0.014388881623744965\n",
      "Epoch 179, Batch Loss: 0.00762088131159544\n",
      "Epoch 180, Batch Loss: 0.014630376361310482\n",
      "Epoch 181, Batch Loss: 0.02174987643957138\n",
      "Epoch 181, Validation Loss: 0.021749859675765038\n",
      "Epoch 182, Batch Loss: 0.011353302747011185\n",
      "Epoch 183, Batch Loss: 0.011822676286101341\n",
      "Epoch 184, Batch Loss: 0.012761903926730156\n",
      "Epoch 185, Batch Loss: 0.015105389058589935\n",
      "Epoch 186, Batch Loss: 0.04577517881989479\n",
      "Epoch 187, Batch Loss: 0.016744855791330338\n",
      "Epoch 188, Batch Loss: 0.011104211211204529\n",
      "Epoch 189, Batch Loss: 0.024836089462041855\n",
      "Epoch 190, Batch Loss: 0.018598029389977455\n",
      "Epoch 191, Batch Loss: 0.013815527781844139\n",
      "Epoch 191, Validation Loss: 0.013815523125231266\n",
      "Epoch 192, Batch Loss: 0.01756582036614418\n",
      "Epoch 193, Batch Loss: 0.011857211589813232\n",
      "Epoch 194, Batch Loss: 0.016858011484146118\n",
      "Epoch 195, Batch Loss: 0.011600787751376629\n",
      "Epoch 196, Batch Loss: 0.047689422965049744\n",
      "Epoch 197, Batch Loss: 0.019592367112636566\n",
      "Epoch 198, Batch Loss: 0.028116954490542412\n",
      "Epoch 199, Batch Loss: 0.0132561344653368\n",
      "Epoch 200, Batch Loss: 0.0295014176517725\n",
      "Epoch 201, Batch Loss: 0.027506405487656593\n",
      "Epoch 201, Validation Loss: 0.027506375685334206\n",
      "Epoch 202, Batch Loss: 0.01631588488817215\n",
      "Epoch 203, Batch Loss: 0.013991614803671837\n",
      "Epoch 204, Batch Loss: 0.009269354864954948\n",
      "Epoch 205, Batch Loss: 0.016040513291954994\n",
      "Epoch 206, Batch Loss: 0.012180458754301071\n",
      "Epoch 207, Batch Loss: 0.03848325088620186\n",
      "Epoch 208, Batch Loss: 0.027603041380643845\n",
      "Epoch 209, Batch Loss: 0.017314262688159943\n",
      "Epoch 210, Batch Loss: 0.010684361681342125\n",
      "Epoch 211, Batch Loss: 0.008854868821799755\n",
      "Epoch 211, Validation Loss: 0.008854866027832031\n",
      "Epoch 212, Batch Loss: 0.017646143212914467\n",
      "Epoch 213, Batch Loss: 0.014743274077773094\n",
      "Epoch 214, Batch Loss: 0.022564532235264778\n",
      "Epoch 215, Batch Loss: 0.011329514905810356\n",
      "Epoch 216, Batch Loss: 0.024946218356490135\n",
      "Epoch 217, Batch Loss: 0.015938114374876022\n",
      "Epoch 218, Batch Loss: 0.010816950350999832\n",
      "Epoch 219, Batch Loss: 0.02336791343986988\n",
      "Epoch 220, Batch Loss: 0.014066169038414955\n",
      "Epoch 221, Batch Loss: 0.00843938160687685\n",
      "Epoch 221, Validation Loss: 0.008439378812909126\n",
      "Epoch 222, Batch Loss: 0.01965893805027008\n",
      "Epoch 223, Batch Loss: 0.020564667880535126\n",
      "Epoch 224, Batch Loss: 0.007724280469119549\n",
      "Epoch 225, Batch Loss: 0.011546803638339043\n",
      "Epoch 226, Batch Loss: 0.015386811457574368\n",
      "Epoch 227, Batch Loss: 0.01250431127846241\n",
      "Epoch 228, Batch Loss: 0.012141121551394463\n",
      "Epoch 229, Batch Loss: 0.008155214600265026\n",
      "Epoch 230, Batch Loss: 0.020131364464759827\n",
      "Epoch 231, Batch Loss: 0.012451459653675556\n",
      "Epoch 231, Validation Loss: 0.012451465241611004\n",
      "Epoch 232, Batch Loss: 0.009487551636993885\n",
      "Epoch 233, Batch Loss: 0.012443622574210167\n",
      "Epoch 234, Batch Loss: 0.010623842477798462\n",
      "Epoch 235, Batch Loss: 0.004933585412800312\n",
      "Epoch 236, Batch Loss: 0.008632413111627102\n",
      "Epoch 237, Batch Loss: 0.006883769761770964\n",
      "Epoch 238, Batch Loss: 0.013537687249481678\n",
      "Epoch 239, Batch Loss: 0.015730788931250572\n",
      "Epoch 240, Batch Loss: 0.007394094485789537\n",
      "Epoch 241, Batch Loss: 0.008496267721056938\n",
      "Epoch 241, Validation Loss: 0.008496267721056938\n",
      "Epoch 242, Batch Loss: 0.02695995196700096\n",
      "Epoch 243, Batch Loss: 0.007831087335944176\n",
      "Epoch 244, Batch Loss: 0.0060992129147052765\n",
      "Epoch 245, Batch Loss: 0.0069951326586306095\n",
      "Epoch 246, Batch Loss: 0.018479010090231895\n",
      "Epoch 247, Batch Loss: 0.006944041699171066\n",
      "Epoch 248, Batch Loss: 0.017762593924999237\n",
      "Epoch 249, Batch Loss: 0.012709355913102627\n",
      "Epoch 250, Batch Loss: 0.008779489435255527\n",
      "Epoch 251, Batch Loss: 0.02039376087486744\n",
      "Epoch 251, Validation Loss: 0.02039375528693199\n",
      "Epoch 252, Batch Loss: 0.01811935193836689\n",
      "Epoch 253, Batch Loss: 0.008934247307479382\n",
      "Epoch 254, Batch Loss: 0.006106913089752197\n",
      "Epoch 255, Batch Loss: 0.007433839607983828\n",
      "Epoch 256, Batch Loss: 0.012882965616881847\n",
      "Epoch 257, Batch Loss: 0.008308704011142254\n",
      "Epoch 258, Batch Loss: 0.010106919333338737\n",
      "Epoch 259, Batch Loss: 0.007945711724460125\n",
      "Epoch 260, Batch Loss: 0.005360445007681847\n",
      "Epoch 261, Batch Loss: 0.0066642737947404385\n",
      "Epoch 261, Validation Loss: 0.0066642751917243\n",
      "Epoch 262, Batch Loss: 0.00929661188274622\n",
      "Epoch 263, Batch Loss: 0.00869682151824236\n",
      "Epoch 264, Batch Loss: 0.011656719259917736\n",
      "Epoch 265, Batch Loss: 0.009668059647083282\n",
      "Epoch 266, Batch Loss: 0.021621098741889\n",
      "Epoch 267, Batch Loss: 0.012757551856338978\n",
      "Epoch 268, Batch Loss: 0.010601301677525043\n",
      "Epoch 269, Batch Loss: 0.01681824028491974\n",
      "Epoch 270, Batch Loss: 0.02889709360897541\n",
      "Epoch 271, Batch Loss: 0.005160477012395859\n",
      "Epoch 271, Validation Loss: 0.005160479806363583\n",
      "Epoch 272, Batch Loss: 0.012293852865695953\n",
      "Epoch 273, Batch Loss: 0.006918205879628658\n",
      "Epoch 274, Batch Loss: 0.007739625871181488\n",
      "Epoch 275, Batch Loss: 0.007638323586434126\n",
      "Epoch 276, Batch Loss: 0.01154452096670866\n",
      "Epoch 277, Batch Loss: 0.010963045060634613\n",
      "Epoch 278, Batch Loss: 0.014806680381298065\n",
      "Epoch 279, Batch Loss: 0.016952399164438248\n",
      "Epoch 280, Batch Loss: 0.015914050862193108\n",
      "Epoch 281, Batch Loss: 0.07763581722974777\n",
      "Epoch 281, Validation Loss: 0.07763577252626419\n",
      "Epoch 282, Batch Loss: 0.012211485765874386\n",
      "Epoch 283, Batch Loss: 0.025752652436494827\n",
      "Epoch 284, Batch Loss: 0.008146473206579685\n",
      "Epoch 285, Batch Loss: 0.012421689927577972\n",
      "Epoch 286, Batch Loss: 0.01181728858500719\n",
      "Epoch 287, Batch Loss: 0.00725187174975872\n",
      "Epoch 288, Batch Loss: 0.010955946519970894\n",
      "Epoch 289, Batch Loss: 0.006971562281250954\n",
      "Epoch 290, Batch Loss: 0.008401024155318737\n",
      "Epoch 291, Batch Loss: 0.013233492150902748\n",
      "Epoch 291, Validation Loss: 0.013233489356935024\n",
      "Epoch 292, Batch Loss: 0.004775909706950188\n",
      "Epoch 293, Batch Loss: 0.006193763576447964\n",
      "Epoch 294, Batch Loss: 0.00953980628401041\n",
      "Epoch 295, Batch Loss: 0.014694842509925365\n",
      "Epoch 296, Batch Loss: 0.008198781870305538\n",
      "Epoch 297, Batch Loss: 0.010401417501270771\n",
      "Epoch 298, Batch Loss: 0.0065284427255392075\n",
      "Epoch 299, Batch Loss: 0.023261763155460358\n",
      "Epoch 300, Batch Loss: 0.009293897077441216\n",
      "Epoch 301, Batch Loss: 0.010266246274113655\n",
      "Epoch 301, Validation Loss: 0.010266244411468506\n",
      "Epoch 302, Batch Loss: 0.008190028369426727\n",
      "Epoch 303, Batch Loss: 0.005179448984563351\n",
      "Epoch 304, Batch Loss: 0.010520058684051037\n",
      "Epoch 305, Batch Loss: 0.006546512711793184\n",
      "Epoch 306, Batch Loss: 0.005500535015016794\n",
      "Epoch 307, Batch Loss: 0.00811714306473732\n",
      "Epoch 308, Batch Loss: 0.005124770570546389\n",
      "Epoch 309, Batch Loss: 0.02430085837841034\n",
      "Epoch 310, Batch Loss: 0.0060058762319386005\n",
      "Epoch 311, Batch Loss: 0.005416235886514187\n",
      "Epoch 311, Validation Loss: 0.005416237749159336\n",
      "Epoch 312, Batch Loss: 0.01780177466571331\n",
      "Epoch 313, Batch Loss: 0.006739479955285788\n",
      "Epoch 314, Batch Loss: 0.008182063698768616\n",
      "Epoch 315, Batch Loss: 0.012941516004502773\n",
      "Epoch 316, Batch Loss: 0.004933486692607403\n",
      "Epoch 317, Batch Loss: 0.010285548865795135\n",
      "Epoch 318, Batch Loss: 0.008819603361189365\n",
      "Epoch 319, Batch Loss: 0.011613576672971249\n",
      "Epoch 320, Batch Loss: 0.0049064368940889835\n",
      "Epoch 321, Batch Loss: 0.008690819144248962\n",
      "Epoch 321, Validation Loss: 0.008690828457474709\n",
      "Epoch 322, Batch Loss: 0.018754009157419205\n",
      "Epoch 323, Batch Loss: 0.009318972937762737\n",
      "Epoch 324, Batch Loss: 0.012658089399337769\n",
      "Epoch 325, Batch Loss: 0.01868562214076519\n",
      "Epoch 326, Batch Loss: 0.013617189601063728\n",
      "Epoch 327, Batch Loss: 0.005622910335659981\n",
      "Epoch 328, Batch Loss: 0.007476144935935736\n",
      "Epoch 329, Batch Loss: 0.005575729534029961\n",
      "Epoch 330, Batch Loss: 0.009878754615783691\n",
      "Epoch 331, Batch Loss: 0.01080850139260292\n",
      "Epoch 331, Validation Loss: 0.010808510705828667\n",
      "Epoch 332, Batch Loss: 0.006183646619319916\n",
      "Epoch 333, Batch Loss: 0.00826093927025795\n",
      "Epoch 334, Batch Loss: 0.008016382344067097\n",
      "Epoch 335, Batch Loss: 0.009748049080371857\n",
      "Epoch 336, Batch Loss: 0.0049254558980464935\n",
      "Epoch 337, Batch Loss: 0.003924045246094465\n",
      "Epoch 338, Batch Loss: 0.00823904201388359\n",
      "Epoch 339, Batch Loss: 0.0066300961188972\n",
      "Epoch 340, Batch Loss: 0.007352237123996019\n",
      "Epoch 341, Batch Loss: 0.007075986359268427\n",
      "Epoch 341, Validation Loss: 0.007075985427945852\n",
      "Epoch 342, Batch Loss: 0.006127580534666777\n",
      "Epoch 343, Batch Loss: 0.009781415574252605\n",
      "Epoch 344, Batch Loss: 0.009645464830100536\n",
      "Epoch 345, Batch Loss: 0.010096728801727295\n",
      "Epoch 346, Batch Loss: 0.014475556090474129\n",
      "Epoch 347, Batch Loss: 0.007063761819154024\n",
      "Epoch 348, Batch Loss: 0.026515675708651543\n",
      "Epoch 349, Batch Loss: 0.010838798247277737\n",
      "Epoch 350, Batch Loss: 0.008968422189354897\n",
      "Epoch 351, Batch Loss: 0.03800195828080177\n",
      "Epoch 351, Validation Loss: 0.03800199180841446\n",
      "Epoch 352, Batch Loss: 0.015901723876595497\n",
      "Epoch 353, Batch Loss: 0.010507028549909592\n",
      "Epoch 354, Batch Loss: 0.006597280502319336\n",
      "Epoch 355, Batch Loss: 0.00676987087354064\n",
      "Epoch 356, Batch Loss: 0.03687543421983719\n",
      "Epoch 357, Batch Loss: 0.006305990740656853\n",
      "Epoch 358, Batch Loss: 0.0044226753525435925\n",
      "Epoch 359, Batch Loss: 0.004429461434483528\n",
      "Epoch 360, Batch Loss: 0.0034674156922847033\n",
      "Epoch 361, Batch Loss: 0.009600849822163582\n",
      "Epoch 361, Validation Loss: 0.009600858204066753\n",
      "Epoch 362, Batch Loss: 0.0072144619189202785\n",
      "Epoch 363, Batch Loss: 0.009635146707296371\n",
      "Epoch 364, Batch Loss: 0.0077957347966730595\n",
      "Epoch 365, Batch Loss: 0.006206936202943325\n",
      "Epoch 366, Batch Loss: 0.00850720889866352\n",
      "Epoch 367, Batch Loss: 0.014253630302846432\n",
      "Epoch 368, Batch Loss: 0.007930222898721695\n",
      "Epoch 369, Batch Loss: 0.007091918960213661\n",
      "Epoch 370, Batch Loss: 0.004197975620627403\n",
      "Epoch 371, Batch Loss: 0.004748957231640816\n",
      "Epoch 371, Validation Loss: 0.004748960491269827\n",
      "Epoch 372, Batch Loss: 0.009166698902845383\n",
      "Epoch 373, Batch Loss: 0.009592290967702866\n",
      "Epoch 374, Batch Loss: 0.0057097808457911015\n",
      "Epoch 375, Batch Loss: 0.01114649698138237\n",
      "Epoch 376, Batch Loss: 0.006309501361101866\n",
      "Epoch 377, Batch Loss: 0.018612755462527275\n",
      "Epoch 378, Batch Loss: 0.005918287672102451\n",
      "Epoch 379, Batch Loss: 0.007859894074499607\n",
      "Epoch 380, Batch Loss: 0.008667387999594212\n",
      "Epoch 381, Batch Loss: 0.008258345536887646\n",
      "Epoch 381, Validation Loss: 0.008258342742919922\n",
      "Epoch 382, Batch Loss: 0.00864302646368742\n",
      "Epoch 383, Batch Loss: 0.0034272163175046444\n",
      "Epoch 384, Batch Loss: 0.020969606935977936\n",
      "Epoch 385, Batch Loss: 0.005793066695332527\n",
      "Epoch 386, Batch Loss: 0.011219865642488003\n",
      "Epoch 387, Batch Loss: 0.01076083816587925\n",
      "Epoch 388, Batch Loss: 0.004847023636102676\n",
      "Epoch 389, Batch Loss: 0.010138466022908688\n",
      "Epoch 390, Batch Loss: 0.009005063213407993\n",
      "Epoch 391, Batch Loss: 0.007174811791628599\n",
      "Epoch 391, Validation Loss: 0.00717480992898345\n",
      "Epoch 392, Batch Loss: 0.014814088121056557\n",
      "Epoch 393, Batch Loss: 0.03414084389805794\n",
      "Epoch 394, Batch Loss: 0.009067648090422153\n",
      "Epoch 395, Batch Loss: 0.01604367047548294\n",
      "Epoch 396, Batch Loss: 0.006163721438497305\n",
      "Epoch 397, Batch Loss: 0.03353286534547806\n",
      "Epoch 398, Batch Loss: 0.0039030732586979866\n",
      "Epoch 399, Batch Loss: 0.01397567056119442\n",
      "Epoch 400, Batch Loss: 0.00994010828435421\n",
      "Epoch 401, Batch Loss: 0.006940776482224464\n",
      "Epoch 401, Validation Loss: 0.006940779741853476\n",
      "Epoch 402, Batch Loss: 0.01180475577712059\n",
      "Epoch 403, Batch Loss: 0.009691123850643635\n",
      "Epoch 404, Batch Loss: 0.00937890075147152\n",
      "Epoch 405, Batch Loss: 0.006504637189209461\n",
      "Epoch 406, Batch Loss: 0.01372522208839655\n",
      "Epoch 407, Batch Loss: 0.009286249987781048\n",
      "Epoch 408, Batch Loss: 0.007630678825080395\n",
      "Epoch 409, Batch Loss: 0.011737633496522903\n",
      "Epoch 410, Batch Loss: 0.006944071967154741\n",
      "Epoch 411, Batch Loss: 0.013075067661702633\n",
      "Epoch 411, Validation Loss: 0.013075060211122036\n",
      "Epoch 412, Batch Loss: 0.0034259334206581116\n",
      "Epoch 413, Batch Loss: 0.005490637384355068\n",
      "Epoch 414, Batch Loss: 0.02619756944477558\n",
      "Epoch 415, Batch Loss: 0.00938732735812664\n",
      "Epoch 416, Batch Loss: 0.013001542538404465\n",
      "Epoch 417, Batch Loss: 0.005857276730239391\n",
      "Epoch 418, Batch Loss: 0.007162219379097223\n",
      "Epoch 419, Batch Loss: 0.005122472997754812\n",
      "Epoch 420, Batch Loss: 0.006879375781863928\n",
      "Epoch 421, Batch Loss: 0.009831218980252743\n",
      "Epoch 421, Validation Loss: 0.009831207804381847\n",
      "Epoch 422, Batch Loss: 0.006204741541296244\n",
      "Epoch 423, Batch Loss: 0.005852588452398777\n",
      "Epoch 424, Batch Loss: 0.016074471175670624\n",
      "Epoch 425, Batch Loss: 0.011363603174686432\n",
      "Epoch 426, Batch Loss: 0.08960819989442825\n",
      "Epoch 427, Batch Loss: 0.006672746501863003\n",
      "Epoch 428, Batch Loss: 0.007154890801757574\n",
      "Epoch 429, Batch Loss: 0.004630471114069223\n",
      "Epoch 430, Batch Loss: 0.012634670361876488\n",
      "Epoch 431, Batch Loss: 0.0059761968441307545\n",
      "Epoch 431, Validation Loss: 0.005976199172437191\n",
      "Epoch 432, Batch Loss: 0.005233207251876593\n",
      "Epoch 433, Batch Loss: 0.00858372263610363\n",
      "Epoch 434, Batch Loss: 0.006759425159543753\n",
      "Epoch 435, Batch Loss: 0.005923422519117594\n",
      "Epoch 436, Batch Loss: 0.011623366735875607\n",
      "Epoch 437, Batch Loss: 0.005291815381497145\n",
      "Epoch 438, Batch Loss: 0.00680468138307333\n",
      "Epoch 439, Batch Loss: 0.006494032684713602\n",
      "Epoch 440, Batch Loss: 0.05043669790029526\n",
      "Epoch 441, Batch Loss: 0.012837830930948257\n",
      "Epoch 441, Validation Loss: 0.012837830930948257\n",
      "Epoch 442, Batch Loss: 0.01820286735892296\n",
      "Epoch 443, Batch Loss: 0.008525067940354347\n",
      "Epoch 444, Batch Loss: 0.00974369514733553\n",
      "Epoch 445, Batch Loss: 0.009194652549922466\n",
      "Epoch 446, Batch Loss: 0.0042463936842978\n",
      "Epoch 447, Batch Loss: 0.009364348836243153\n",
      "Epoch 448, Batch Loss: 0.0051986221224069595\n",
      "Epoch 449, Batch Loss: 0.007173458579927683\n",
      "Epoch 450, Batch Loss: 0.007972714491188526\n",
      "Epoch 451, Batch Loss: 0.009547392837703228\n",
      "Epoch 451, Validation Loss: 0.009547396562993526\n",
      "Epoch 452, Batch Loss: 0.007352203130722046\n",
      "Epoch 453, Batch Loss: 0.004510848317295313\n",
      "Epoch 454, Batch Loss: 0.015152622945606709\n",
      "Epoch 455, Batch Loss: 0.005403224378824234\n",
      "Epoch 456, Batch Loss: 0.010419026017189026\n",
      "Epoch 457, Batch Loss: 0.004381287377327681\n",
      "Epoch 458, Batch Loss: 0.005140217486768961\n",
      "Epoch 459, Batch Loss: 0.005187456030398607\n",
      "Epoch 460, Batch Loss: 0.009600645862519741\n",
      "Epoch 461, Batch Loss: 0.0033606213983148336\n",
      "Epoch 461, Validation Loss: 0.0033606188371777534\n",
      "Epoch 462, Batch Loss: 0.01671737991273403\n",
      "Epoch 463, Batch Loss: 0.004219702910631895\n",
      "Epoch 464, Batch Loss: 0.01150090154260397\n",
      "Epoch 465, Batch Loss: 0.00971205160021782\n",
      "Epoch 466, Batch Loss: 0.020304350182414055\n",
      "Epoch 467, Batch Loss: 0.007636785972863436\n",
      "Epoch 468, Batch Loss: 0.009594639763236046\n",
      "Epoch 469, Batch Loss: 0.015107512474060059\n",
      "Epoch 470, Batch Loss: 0.007300029508769512\n",
      "Epoch 471, Batch Loss: 0.008724579587578773\n",
      "Epoch 471, Validation Loss: 0.008724573999643326\n",
      "Epoch 472, Batch Loss: 0.005276195239275694\n",
      "Epoch 473, Batch Loss: 0.005841739010065794\n",
      "Epoch 474, Batch Loss: 0.01528098713606596\n",
      "Epoch 475, Batch Loss: 0.03100225143134594\n",
      "Epoch 476, Batch Loss: 0.01699502021074295\n",
      "Epoch 477, Batch Loss: 0.010976647958159447\n",
      "Epoch 478, Batch Loss: 0.015485142357647419\n",
      "Epoch 479, Batch Loss: 0.008224517107009888\n",
      "Epoch 480, Batch Loss: 0.007060868199914694\n",
      "Epoch 481, Batch Loss: 0.011084620840847492\n",
      "Epoch 481, Validation Loss: 0.011084617115557194\n",
      "Epoch 482, Batch Loss: 0.010160769335925579\n",
      "Epoch 483, Batch Loss: 0.006923686247318983\n",
      "Epoch 484, Batch Loss: 0.005988165270537138\n",
      "Epoch 485, Batch Loss: 0.005715892184525728\n",
      "Epoch 486, Batch Loss: 0.012979879975318909\n",
      "Epoch 487, Batch Loss: 0.0058753532357513905\n",
      "Epoch 488, Batch Loss: 0.0054878718219697475\n",
      "Epoch 489, Batch Loss: 0.007528264075517654\n",
      "Epoch 490, Batch Loss: 0.010799516923725605\n",
      "Epoch 491, Batch Loss: 0.00703800143674016\n",
      "Epoch 491, Validation Loss: 0.007037998642772436\n",
      "Epoch 492, Batch Loss: 0.006103615276515484\n",
      "Epoch 493, Batch Loss: 0.006337795406579971\n",
      "Epoch 494, Batch Loss: 0.010746816173195839\n",
      "Epoch 495, Batch Loss: 0.009032641537487507\n",
      "Epoch 496, Batch Loss: 0.007702517788857222\n",
      "Epoch 497, Batch Loss: 0.010120194405317307\n",
      "Epoch 498, Batch Loss: 0.006108907517045736\n",
      "Epoch 499, Batch Loss: 0.005684074480086565\n",
      "Epoch 500, Batch Loss: 0.009541790001094341\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torchvision.ops import box_convert\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 500\n",
    "warmup_epochs = 50\n",
    "evaluate_every = 10\n",
    "\n",
    "# Create your SSD Lite model\n",
    "model = ssdlite320_mobilenet_v3_large(weights= \"SSDLite320_MobileNet_V3_Large_Weights.DEFAULT\")\n",
    "\n",
    "# Define your loss function and optimizer\n",
    "criterion = generalized_box_iou_loss\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.015, momentum=0.9)\n",
    "# Set up the cosine annealing learning rate scheduler\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs - warmup_epochs)\n",
    "\n",
    "writer = tensorboard.SummaryWriter()\n",
    "\n",
    "# Define the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        images, targets = data        \n",
    "        \n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        #targets = [{k: v.to(device).long() if k == \"labels\" else v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = loss_dict[\"classification\"] + loss_dict[\"bbox_regression\"]\n",
    "\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch + 1}, Batch Loss: {losses}\")\n",
    "        \n",
    "            # Update the learning rate\n",
    "    scheduler.step()\n",
    "        # Log the training loss to Tensorboard\n",
    "    writer.add_scalar('Loss/train', losses.item(), epoch)\n",
    "   \n",
    "    \n",
    "      # Evaluate on the validation set every 'evaluate_every' epochs\n",
    "    if epoch % evaluate_every == 0:\n",
    "        with torch.no_grad():\n",
    "            val_losses = 0.0\n",
    "            for images, targets in val_loader:\n",
    "                images = [img.to(device) for img in images]\n",
    "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "                # Forward pass\n",
    "                val_loss_dict = model(images, targets)\n",
    "                \n",
    "                val_losses += val_loss_dict[\"classification\"] + loss_dict[\"bbox_regression\"]\n",
    "\n",
    "            avg_val_loss = val_losses / len(val_loader)\n",
    "\n",
    "            # Log the validation loss to Tensorboard\n",
    "            writer.add_scalar('Loss/val', avg_val_loss, epoch)\n",
    "            print(f\"Epoch {epoch + 1}, Validation Loss: {avg_val_loss.item()}\")\n",
    "            torch.save(model.state_dict(), f\"models/ssdlite_cows_model_e{epoch}.pth\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'models/ssdlite_cows_model_500e.pth')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7891b921-d3aa-4464-aef8-bb9a25fac760",
   "metadata": {},
   "source": [
    "Epoch 1, Batch Loss: 2.1624016761779785\n",
    "Epoch 1, Validation Loss: 2.162400245666504\n",
    "Epoch 2, Batch Loss: 2.5404789447784424\n",
    "Epoch 3, Batch Loss: 2.3300535678863525\n",
    "Epoch 4, Batch Loss: 2.4105634689331055\n",
    "Epoch 5, Batch Loss: 2.106541156768799\n",
    "Epoch 6, Batch Loss: 2.1915464401245117\n",
    "Epoch 6, Validation Loss: 2.191544771194458\n",
    "Epoch 7, Batch Loss: 2.015090227127075\n",
    "Epoch 8, Batch Loss: 1.9103816747665405\n",
    "Epoch 9, Batch Loss: 2.6565113067626953\n",
    "Epoch 10, Batch Loss: 1.546593189239502\n",
    "Epoch 11, Batch Loss: 1.8698519468307495\n",
    "Epoch 11, Validation Loss: 1.8698517084121704\n",
    "Epoch 12, Batch Loss: 2.277885675430298\n",
    "Epoch 13, Batch Loss: 1.0736545324325562\n",
    "Epoch 14, Batch Loss: 1.4011064767837524\n",
    "Epoch 15, Batch Loss: 0.7211280465126038\n",
    "Epoch 16, Batch Loss: 1.552635669708252\n",
    "Epoch 16, Validation Loss: 1.5526351928710938\n",
    "Epoch 17, Batch Loss: 1.8985334634780884\n",
    "Epoch 18, Batch Loss: 1.0585334300994873\n",
    "Epoch 19, Batch Loss: 0.6549271941184998\n",
    "Epoch 20, Batch Loss: 0.6437249183654785\n",
    "Epoch 21, Batch Loss: 0.775252640247345\n",
    "Epoch 21, Validation Loss: 0.7752530574798584\n",
    "Epoch 22, Batch Loss: 0.5523011684417725\n",
    "Epoch 23, Batch Loss: 0.5858114957809448\n",
    "Epoch 24, Batch Loss: 0.8299723863601685\n",
    "Epoch 25, Batch Loss: 0.6758676171302795\n",
    "Epoch 26, Batch Loss: 0.37474769353866577\n",
    "Epoch 26, Validation Loss: 0.3747478425502777\n",
    "Epoch 27, Batch Loss: 0.4907954931259155\n",
    "Epoch 28, Batch Loss: 0.28217196464538574\n",
    "Epoch 29, Batch Loss: 0.4357515573501587\n",
    "Epoch 30, Batch Loss: 0.1401309370994568\n",
    "Epoch 31, Batch Loss: 0.1647355854511261\n",
    "Epoch 31, Validation Loss: 0.16473571956157684\n",
    "Epoch 32, Batch Loss: 0.15228570997714996\n",
    "Epoch 33, Batch Loss: 0.11974284052848816\n",
    "Epoch 34, Batch Loss: 0.16989053785800934\n",
    "Epoch 35, Batch Loss: 0.11060953140258789\n",
    "Epoch 36, Batch Loss: 0.23397697508335114\n",
    "Epoch 36, Validation Loss: 0.23397698998451233\n",
    "Epoch 37, Batch Loss: 0.10249509662389755\n",
    "Epoch 38, Batch Loss: 0.1783769279718399\n",
    "Epoch 39, Batch Loss: 0.27197781205177307\n",
    "Epoch 40, Batch Loss: 0.060597095638513565\n",
    "Epoch 41, Batch Loss: 0.2649507522583008\n",
    "Epoch 41, Validation Loss: 0.2649504840373993\n",
    "Epoch 42, Batch Loss: 0.08115487545728683\n",
    "Epoch 43, Batch Loss: 0.10914859920740128\n",
    "Epoch 44, Batch Loss: 0.08243907243013382\n",
    "Epoch 45, Batch Loss: 0.08893191814422607\n",
    "Epoch 46, Batch Loss: 0.05804527923464775\n",
    "Epoch 46, Validation Loss: 0.05804533138871193\n",
    "Epoch 47, Batch Loss: 0.3471693694591522\n",
    "Epoch 48, Batch Loss: 0.0683818981051445\n",
    "Epoch 49, Batch Loss: 0.08555656671524048\n",
    "Epoch 50, Batch Loss: 0.08576574176549911\n",
    "Epoch 51, Batch Loss: 0.10381092131137848\n",
    "Epoch 51, Validation Loss: 0.10381089895963669\n",
    "Epoch 52, Batch Loss: 0.047768350690603256\n",
    "Epoch 53, Batch Loss: 0.08078217506408691\n",
    "Epoch 54, Batch Loss: 0.04985416680574417\n",
    "Epoch 55, Batch Loss: 0.04265188053250313\n",
    "Epoch 56, Batch Loss: 0.0492166243493557\n",
    "Epoch 56, Validation Loss: 0.04921656474471092\n",
    "Epoch 57, Batch Loss: 0.044927652925252914\n",
    "Epoch 58, Batch Loss: 0.04169200733304024\n",
    "Epoch 59, Batch Loss: 0.09698423743247986\n",
    "Epoch 60, Batch Loss: 0.0698719173669815\n",
    "Epoch 61, Batch Loss: 0.08348662406206131\n",
    "Epoch 61, Validation Loss: 0.08348657935857773\n",
    "Epoch 62, Batch Loss: 0.06443209201097488\n",
    "Epoch 63, Batch Loss: 0.07328645884990692\n",
    "Epoch 64, Batch Loss: 0.025384951382875443\n",
    "Epoch 65, Batch Loss: 0.05251451954245567\n",
    "Epoch 66, Batch Loss: 0.031014230102300644\n",
    "Epoch 66, Validation Loss: 0.031014209613204002\n",
    "Epoch 67, Batch Loss: 0.02969858981668949\n",
    "Epoch 68, Batch Loss: 0.03139123320579529\n",
    "Epoch 69, Batch Loss: 0.040941447019577026\n",
    "Epoch 70, Batch Loss: 0.04247945919632912\n",
    "Epoch 71, Batch Loss: 0.05030020698904991\n",
    "Epoch 71, Validation Loss: 0.05030016601085663\n",
    "Epoch 72, Batch Loss: 0.04001690074801445\n",
    "Epoch 73, Batch Loss: 0.12307494133710861\n",
    "Epoch 74, Batch Loss: 0.06682360172271729\n",
    "Epoch 75, Batch Loss: 0.050678908824920654\n",
    "Epoch 76, Batch Loss: 0.02051289565861225\n",
    "Epoch 76, Validation Loss: 0.020512908697128296\n",
    "Epoch 77, Batch Loss: 0.024961793795228004\n",
    "Epoch 78, Batch Loss: 0.021898822858929634\n",
    "Epoch 79, Batch Loss: 0.07592733949422836\n",
    "Epoch 80, Batch Loss: 0.07748715579509735\n",
    "Epoch 81, Batch Loss: 0.09132784605026245\n",
    "Epoch 81, Validation Loss: 0.09132777899503708\n",
    "Epoch 82, Batch Loss: 0.03422539308667183\n",
    "Epoch 83, Batch Loss: 0.020913509652018547\n",
    "Epoch 84, Batch Loss: 0.05341031774878502\n",
    "Epoch 85, Batch Loss: 0.07039040327072144\n",
    "Epoch 86, Batch Loss: 0.2477205991744995\n",
    "Epoch 86, Validation Loss: 0.24772068858146667\n",
    "Epoch 87, Batch Loss: 0.05110390484333038\n",
    "Epoch 88, Batch Loss: 0.10275000333786011\n",
    "Epoch 89, Batch Loss: 0.03458305448293686\n",
    "Epoch 90, Batch Loss: 0.08632629364728928\n",
    "Epoch 91, Batch Loss: 0.029491905122995377\n",
    "Epoch 91, Validation Loss: 0.029491903260350227\n",
    "Epoch 92, Batch Loss: 0.017571212723851204\n",
    "Epoch 93, Batch Loss: 0.056979794055223465\n",
    "Epoch 94, Batch Loss: 0.048410408198833466\n",
    "Epoch 95, Batch Loss: 0.044813450425863266\n",
    "Epoch 96, Batch Loss: 0.03621485456824303\n",
    "Epoch 96, Validation Loss: 0.036214832216501236\n",
    "Epoch 97, Batch Loss: 0.030982185155153275\n",
    "Epoch 98, Batch Loss: 0.04555738717317581\n",
    "Epoch 99, Batch Loss: 0.02477923035621643\n",
    "Epoch 100, Batch Loss: 0.02978690154850483"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
